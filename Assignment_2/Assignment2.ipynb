{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WeCeITXoxLf",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignment 2\n",
    "\n",
    "**Credits**: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\n",
    "\n",
    "**Keywords**: Human Value Detection, Multi-label classification, Transformers, BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "# Contact\n",
    "\n",
    "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
    "\n",
    "Teaching Assistants:\n",
    "\n",
    "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
    "* Eleonora Mancini -> e.mancini@unibo.it\n",
    "\n",
    "Professor:\n",
    "\n",
    "* Paolo Torroni -> p.torroni@unibo.it"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Instructions\n",
    "\n",
    "* **Download** the specificed training, validation, and test files.\n",
    "* **Encode** split files into a pandas.DataFrame object.\n",
    "* For each split, **merge** the arguments and labels dataframes into a single dataframe.\n",
    "* **Merge** level 2 annotations to level 3 categories."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install seaborn\n",
    "!pip install transformers\n",
    "!pip install torcheval\n",
    "!pip install gdown"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "You are tasked to address the [Human Value Detection challenge](https://aclanthology.org/2022.acl-long.306/)."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problem definition\n",
    "\n",
    "Arguments are paired with their conveyed human values.\n",
    "\n",
    "Arguments are in the form of **premise** $\\rightarrow$ **conclusion**.\n",
    "\n",
    "### Example:\n",
    "\n",
    "**Premise**: *``fast food should be banned because it is really bad for your health and is costly''*\n",
    "\n",
    "**Conclusion**: *``We should ban fast food''*\n",
    "\n",
    "**Stance**: *in favour of*"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center>\n",
    "    <img src=\"images/human_values.png\" alt=\"human values\" />\n",
    "</center>"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# [Task 1 - 0.5 points] Corpus\n",
    "\n",
    "Check the official page of the challenge [here](https://touche.webis.de/semeval23/touche23-web/).\n",
    "\n",
    "The challenge offers several corpora for evaluation and testing.\n",
    "\n",
    "You are going to work with the standard training, validation, and test splits.\n",
    "\n",
    "#### Arguments\n",
    "* arguments-training.tsv\n",
    "* arguments-validation.tsv\n",
    "* arguments-test.tsv\n",
    "\n",
    "#### Human values\n",
    "* labels-training.tsv\n",
    "* labels-validation.tsv\n",
    "* labels-test.tsv"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!curl https://zenodo.org/records/8248658/files/arguments-training.tsv?download=1 -o arguments-training.tsv\n",
    "!curl https://zenodo.org/records/8248658/files/arguments-validation.tsv?download=1 -o arguments-validation.tsv\n",
    "!curl https://zenodo.org/records/8248658/files/arguments-test.tsv?download=1 -o arguments-test.tsv\n",
    "!curl https://zenodo.org/records/8248658/files/labels-training.tsv?download=1 -o labels-training.tsv\n",
    "!curl https://zenodo.org/records/8248658/files/labels-validation.tsv?download=1 -o labels-validation.tsv\n",
    "!curl https://zenodo.org/records/8248658/files/labels-test.tsv?download=1 -o labels-test.tsv"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example\n",
    "\n",
    "#### arguments-*.tsv\n",
    "```\n",
    "\n",
    "Argument ID    A01005\n",
    "\n",
    "Conclusion     We should ban fast food\n",
    "\n",
    "Stance         in favor of\n",
    "\n",
    "Premise        fast food should be banned because it is really bad for your health and is costly.\n",
    "```\n",
    "\n",
    "#### labels-*.tsv\n",
    "\n",
    "```\n",
    "Argument ID                A01005\n",
    "\n",
    "Self-direction: thought    0\n",
    "Self-direction: action     0\n",
    "...\n",
    "Universalism: objectivity: 0\n",
    "```"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Splits\n",
    "\n",
    "The standard splits contain\n",
    "\n",
    "   * **Train**: 5393 arguments\n",
    "   * **Validation**: 1896 arguments\n",
    "   * **Test**: 1576 arguments"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Annotations\n",
    "\n",
    "In this assignment, you are tasked to address a multi-label classification problem.\n",
    "\n",
    "You are going to consider **level 3** categories:\n",
    "\n",
    "* Openness to change\n",
    "* Self-enhancement\n",
    "* Conversation\n",
    "* Self-transcendence"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**How to do that?**\n",
    "\n",
    "You have to merge (**logical OR**) annotations of level 2 categories belonging to the same level 3 category.\n",
    "\n",
    "**Pay attention to shared level 2 categories** (e.g., Hedonism). $\\rightarrow$ [see Table 1 in the original paper.](https://aclanthology.org/2022.acl-long.306/)\n",
    "\n",
    "#### Example\n",
    "\n",
    "```\n",
    "Self-direction: thought:    0\n",
    "Self-direction: action:     1\n",
    "Stimulation:                0\n",
    "Hedonism:                   1\n",
    "\n",
    "Openess to change           1\n",
    "```"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torcheval.metrics.functional import binary_f1_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import gdown"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "gdown.download('https://drive.usercontent.google.com/u/0/uc?id=17Nb2c918XvENe6JoP65cX95x27zXJT77&export=download', 'utils.tar.gz', quiet=False)\n",
    "\n",
    "!tar -xf utils.tar.gz"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from file_reader import import_features, import_labels\n",
    "from dataframe_modifier import modify_stance, create_third_level_labels\n",
    "from CustomDataset import CustomDataset\n",
    "from network_trainer import train, evaluate_model\n",
    "from plots import generate_summary, generate_precision_recall_curve, generate_confusion_matrix, \\\n",
    "    generate_f1_scores_table, generate_bar_plot_with_f1_scores, generate_training_history_plots, \\\n",
    "    show_some_misclassified_examples, generate_bar_plot, generate_correlation_heatmap\n",
    "\n",
    "\n",
    "save_best_models = False\n",
    "# If `load_best_models` is set to True, the notebook will automatically try to download\n",
    "# the models' weights from Google Drive\n",
    "load_best_models = True\n",
    "models_load_link = 'https://drive.usercontent.google.com/download?id=13xq53-QPIqb-SQAURvPL0v-YvzW6iPxh&export=download&confirm=t&uuid=75139e51-946d-4242-8a66-b50a58c05e5d'\n",
    "models_load_path = 'best_models.tar'\n",
    "models_save_path = 'best_models.tar'\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_id = 1\n",
    "models_dict = [{'name': 'bert-base-uncased', 'head_size': 768},\n",
    "               {'name': 'roberta-base', 'head_size': 768},\n",
    "               {'name': 'roberta-large', 'head_size': 1024}]\n",
    "\n",
    "initializer_seed = 111\n",
    "# TODO: before running with the good seeds fix y-axis limits of plots\n",
    "seeds = [339, 1234, 4321]\n",
    "\n",
    "num_epochs = 10"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "random.seed(initializer_seed)\n",
    "np.random.seed(initializer_seed)\n",
    "torch.manual_seed(initializer_seed)\n",
    "\n",
    "train_dataframe, validation_dataframe, test_dataframe = import_features()\n",
    "lab_train_dataframe, lab_validation_dataframe, lab_test_dataframe = import_labels()\n",
    "modify_stance(train_dataframe, validation_dataframe, test_dataframe)\n",
    "\n",
    "third_level_train_dataframe, third_level_validation_dataframe, third_level_test_dataframe = \\\n",
    "    create_third_level_labels(lab_train_dataframe, lab_validation_dataframe, lab_test_dataframe)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(models_dict[model_id]['name'])\n",
    "\n",
    "# Generate datasets and dataloaders\n",
    "training_set = CustomDataset(train_dataframe, third_level_train_dataframe, tokenizer)\n",
    "validation_set = CustomDataset(validation_dataframe, third_level_validation_dataframe, tokenizer)\n",
    "test_set = CustomDataset(test_dataframe, third_level_test_dataframe, tokenizer)\n",
    "\n",
    "training_loader = DataLoader(training_set, batch_size=16, shuffle=True)\n",
    "validation_loader = DataLoader(validation_set, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=16, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "generate_bar_plot(third_level_train_dataframe, third_level_validation_dataframe, third_level_test_dataframe)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "generate_correlation_heatmap(third_level_train_dataframe, third_level_validation_dataframe, third_level_test_dataframe)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 2 - 2.0 points] Model definition\n",
    "\n",
    "You are tasked to define several neural models for multi-label classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"images/model_schema.png\" alt=\"model_schema\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* **Baseline**: implement a random uniform classifier (an individual classifier per category).\n",
    "* **Baseline**: implement a majority classifier (an individual classifier per category).\n",
    "\n",
    "<br/>\n",
    "\n",
    "* **BERT w/ C**: define a BERT-based classifier that receives an argument **conclusion** as input.\n",
    "* **BERT w/ CP**: add argument **premise** as an additional input.\n",
    "* **BERT w/ CPS**: add argument premise-to-conclusion **stance** as an additional input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Notes\n",
    "\n",
    "**Do not mix models**. Each model has its own instructions.\n",
    "\n",
    "You are **free** to select the BERT-based model card from huggingface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Examples\n",
    "\n",
    "```\n",
    "bert-base-uncased\n",
    "prajjwal1/bert-tiny\n",
    "distilbert-base-uncased\n",
    "roberta-base\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### BERT w/ C\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/bert_c.png\" alt=\"BERT w/ C\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### BERT w/ CP\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/bert_cp.png\" alt=\"BERT w/ CP\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### BERT w/ CPS\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/bert_cps.png\" alt=\"BERT w/ CPS\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Input concatenation\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/input_merging.png\" alt=\"Input merging\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Notes\n",
    "\n",
    "The **stance** input has to be encoded into a numerical format.\n",
    "\n",
    "You **should** use the same model instance to encode **premise** and **conclusion** inputs."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def random_uniform_classifier(test_labels):\n",
    "    test_labels = test_labels.values\n",
    "    n_instances = test_labels.shape[0]\n",
    "    n_classes = test_labels.shape[1]\n",
    "\n",
    "    predictions = np.random.randint(2, size=(n_instances, n_classes))\n",
    "    predictions = torch.Tensor(predictions)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def majority_classifier(training_labels, test_labels):\n",
    "    training_labels = training_labels.values\n",
    "    test_labels = test_labels.values\n",
    "    n_instances = test_labels.shape[0]\n",
    "    n_classes = test_labels.shape[1]\n",
    "\n",
    "    predictions = np.zeros((n_instances, n_classes))\n",
    "    for i in range(n_classes):\n",
    "        if np.sum(training_labels[:, i]) >= len(training_labels) / 2:\n",
    "            predictions[:, i] = 1\n",
    "    predictions = torch.Tensor(predictions)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def one_baseline(test_labels):\n",
    "    test_labels = test_labels.values\n",
    "    n_instances = test_labels.shape[0]\n",
    "    n_classes = test_labels.shape[1]\n",
    "\n",
    "    predictions = np.ones((n_instances, n_classes))\n",
    "    predictions = torch.Tensor(predictions)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Baselines\n",
    "# 1. Random\n",
    "predictions_random = random_uniform_classifier(third_level_test_dataframe)\n",
    "val_predictions_random = random_uniform_classifier(third_level_validation_dataframe)\n",
    "\n",
    "# 2. Majority\n",
    "predictions_majority = majority_classifier(third_level_train_dataframe, third_level_test_dataframe)\n",
    "val_predictions_majority = majority_classifier(third_level_train_dataframe, third_level_validation_dataframe)\n",
    "\n",
    "# 3. 1-baseline\n",
    "predictions_one = one_baseline(third_level_test_dataframe)\n",
    "val_predictions_one = one_baseline(third_level_validation_dataframe)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ClassifierC(nn.Module):\n",
    "    def __init__(self, name, head_size):\n",
    "        super(ClassifierC, self).__init__()\n",
    "        self.name_ = 'C'\n",
    "        self.embedder = AutoModel.from_pretrained(name)\n",
    "        for param in self.embedder.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.linear = nn.Linear(head_size, 4)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[0]\n",
    "        attention_mask = x['attention_mask'].unsqueeze(-1)\n",
    "        x = self.embedder(**x).last_hidden_state\n",
    "        x = x * attention_mask\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.linear(x)\n",
    "        x = (self.tanh(x) + 1) / 2\n",
    "        return x\n",
    "\n",
    "\n",
    "class ClassifierCP(nn.Module):\n",
    "    def __init__(self, name, head_size):\n",
    "        super(ClassifierCP, self).__init__()\n",
    "        self.name_ = 'CP'\n",
    "        self.embedder = AutoModel.from_pretrained(name)\n",
    "        for param in self.embedder.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.linear = nn.Linear(head_size * 2, 4)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x[0]\n",
    "        z = x[1]\n",
    "        attention_mask_y = y['attention_mask'].unsqueeze(-1)\n",
    "        attention_mask_z = z['attention_mask'].unsqueeze(-1)\n",
    "        y = self.embedder(**y).last_hidden_state\n",
    "        z = self.embedder(**z).last_hidden_state\n",
    "        y = y * attention_mask_y\n",
    "        z = z * attention_mask_z\n",
    "        y = y.mean(dim=1)\n",
    "        z = z.mean(dim=1)\n",
    "        x = torch.cat([y, z], 1)\n",
    "        x = self.linear(x)\n",
    "        x = (self.tanh(x) + 1) / 2\n",
    "        return x\n",
    "\n",
    "\n",
    "class ClassifierCPS(nn.Module):\n",
    "    def __init__(self, name, head_size):\n",
    "        super(ClassifierCPS, self).__init__()\n",
    "        self.name_ = 'CPS'\n",
    "        self.embedder = AutoModel.from_pretrained(name)\n",
    "        for param in self.embedder.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.linear = nn.Linear(head_size * 2 + 1, 4)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x[0]\n",
    "        z = x[1]\n",
    "        w = x[2]\n",
    "        attention_mask_y = y['attention_mask'].unsqueeze(-1)\n",
    "        attention_mask_z = z['attention_mask'].unsqueeze(-1)\n",
    "        y = self.embedder(**y).last_hidden_state\n",
    "        z = self.embedder(**z).last_hidden_state\n",
    "        y = y * attention_mask_y\n",
    "        z = z * attention_mask_z\n",
    "        y = y.mean(dim=1)\n",
    "        z = z.mean(dim=1)\n",
    "        x = torch.cat([y, z, w.reshape((y.shape[0], 1))], 1)\n",
    "        x = self.linear(x)\n",
    "        x = (self.tanh(x) + 1) / 2\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 3 - 0.5 points] Metrics\n",
    "\n",
    "Before training the models, you are tasked to define the evaluation metrics for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Evaluate your models using per-category binary F1-score.\n",
    "* Compute the average binary F1-score over all categories (macro F1-score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "You start with individual predictions ($\\rightarrow$ samples).\n",
    "\n",
    "```\n",
    "Openess to change:    0 0 1 0 1 1 0 ...\n",
    "Self-enhancement:     1 0 0 0 1 0 1 ...\n",
    "Conversation:         0 0 0 1 1 0 1 ...\n",
    "Self-transcendence:   1 1 0 1 0 1 0 ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You compute per-category binary F1-score.\n",
    "\n",
    "```\n",
    "Openess to change F1:    0.35\n",
    "Self-enhancement F1:     0.55\n",
    "Conversation F1:         0.80\n",
    "Self-transcendence F1:   0.21\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You then average per-category scores.\n",
    "```\n",
    "Average F1: ~0.48\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def calculate_f1_score(predictions, targets, verbose=False):\n",
    "    cols = predictions.shape[1]\n",
    "    single_class_scores = torch.zeros(cols)\n",
    "    for i in range(cols):\n",
    "        single_class_scores[i] = binary_f1_score(predictions[:, i], targets[:, i])\n",
    "        if verbose:\n",
    "            print('F1 score for column %d: %.3f' % (i, single_class_scores[i]))\n",
    "    return torch.mean(single_class_scores), single_class_scores"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 4 - 1.0 points] Training and Evaluation\n",
    "\n",
    "You are now tasked to train and evaluate **all** defined models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Train **all** models on the train set.\n",
    "* Evaluate **all** models on the validation set.\n",
    "* Pick **at least** three seeds for robust estimation.\n",
    "* Compute metrics on the validation set.\n",
    "* Report **per-category** and **macro** F1-score for comparison."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Initialize training data structures\n",
    "histories = {}\n",
    "best_models = {}\n",
    "models = {'C': ClassifierC,\n",
    "          'CP': ClassifierCP,\n",
    "          'CPS': ClassifierCPS\n",
    "          }\n",
    "\n",
    "# Train models\n",
    "if not load_best_models:\n",
    "    for seed in seeds:\n",
    "        for model_type, model_class in models.items():\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            print('Set seed to: ', seed)\n",
    "\n",
    "            model = model_class(**models_dict[model_id])\n",
    "            model_trained, history = train(model, training_loader, validation_loader, num_epochs)\n",
    "            if model_type not in histories:\n",
    "                histories[model_type] = history\n",
    "                best_models[model_type] = model_trained\n",
    "            else:\n",
    "                if history['best_val_macro_f1'] > histories[model_type]['best_val_macro_f1']:\n",
    "                    histories[model_type] = history\n",
    "                    best_models[model_type] = model_trained\n",
    "\n",
    "    if save_best_models:\n",
    "        torch.save({\n",
    "            'modelC_state_dict': best_models['C'].state_dict(),\n",
    "            'modelCP_state_dict': best_models['CP'].state_dict(),\n",
    "            'modelCPS_state_dict': best_models['CPS'].state_dict(),\n",
    "            'historyC': histories['C'],\n",
    "            'historyCP': histories['CP'],\n",
    "            'historyCPS': histories['CPS']\n",
    "        }, models_save_path)\n",
    "        print('Models saved successfully to: ', models_save_path, '\\n')\n",
    "else:\n",
    "    gdown.download(models_load_link, models_load_path, quiet=False)\n",
    "\n",
    "    checkpoint = torch.load(models_load_path, map_location=device)\n",
    "    best_models['C'] = ClassifierC(**models_dict[model_id])\n",
    "    best_models['C'].load_state_dict(checkpoint['modelC_state_dict'])\n",
    "    best_models['CP'] = ClassifierCP(**models_dict[model_id])\n",
    "    best_models['CP'].load_state_dict(checkpoint['modelCP_state_dict'])\n",
    "    best_models['CPS'] = ClassifierCPS(**models_dict[model_id])\n",
    "    best_models['CPS'].load_state_dict(checkpoint['modelCPS_state_dict'])\n",
    "    histories['C'] = checkpoint['historyC']\n",
    "    histories['CP'] = checkpoint['historyCP']\n",
    "    histories['CPS'] = checkpoint['historyCPS']\n",
    "    print('Models loaded successfully from: ', models_load_path, '\\n')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validation Macro F1 scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "val_macro_f1_scores = []\n",
    "# Baselines over validation set\n",
    "val_macro_f1_scores.append(\n",
    "    ['random', calculate_f1_score(val_predictions_random, torch.Tensor(third_level_validation_dataframe.values))[0].item()])\n",
    "val_macro_f1_scores.append(\n",
    "    ['majority', calculate_f1_score(val_predictions_majority, torch.Tensor(third_level_validation_dataframe.values))[0].item()])\n",
    "val_macro_f1_scores.append(\n",
    "    ['one', calculate_f1_score(val_predictions_one, torch.Tensor(third_level_validation_dataframe.values))[0].item()])\n",
    "\n",
    "for model_type, history in histories.items():\n",
    "    val_macro_f1_scores.append([model_type, history['best_val_macro_f1']])\n",
    "\n",
    "val_macro_f1_scores = pd.DataFrame(val_macro_f1_scores, columns=['Model', 'Macro F1 score'])\n",
    "val_macro_f1_scores"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation of best models on test set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "outputs_dict = {'random': predictions_random, 'majority': predictions_majority, 'one': predictions_one}\n",
    "labels = third_level_test_dataframe.values\n",
    "crisp_predictions_dict = copy.deepcopy(outputs_dict)\n",
    "\n",
    "summaries = []\n",
    "for model_type, model in best_models.items():\n",
    "    _, _, _, outputs, labels_, crisp_predictions = evaluate_model(model, test_loader, device, verbose=False)\n",
    "    assert np.array_equal(labels, labels_)\n",
    "    outputs_dict[model_type] = outputs\n",
    "    crisp_predictions_dict[model_type] = crisp_predictions\n",
    "\n",
    "    summaries.append(generate_summary(crisp_predictions, labels, verbose=False))"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test Macro F1 scores - Classifier C"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "summaries[0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test Macro F1 scores - Classifier CP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "summaries[1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test Macro F1 scores - Classifier CPS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "summaries[2]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Yet another F1 score table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "f1_score_table = generate_f1_scores_table(outputs_dict, labels, crisp_predictions_dict, verbose=False)\n",
    "f1_score_table"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 5 - 1.0 points] Error Analysis\n",
    "\n",
    "You are tasked to discuss your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* **Compare** classification performance of BERT-based models with respect to baselines.\n",
    "* Discuss **difference in prediction** between the best performing BERT-based model and its variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Notes\n",
    "\n",
    "You can check the [original paper](https://aclanthology.org/2022.acl-long.306/) for suggestions on how to perform comparisons (e.g., plots, tables, etc...)."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "generate_training_history_plots(histories)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "generate_precision_recall_curve(outputs_dict, labels, crisp_predictions_dict)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "generate_confusion_matrix(outputs_dict, labels, crisp_predictions_dict)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "generate_bar_plot_with_f1_scores(outputs_dict, labels, crisp_predictions_dict)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "misclassified_dict = show_some_misclassified_examples(test_dataframe, labels, crisp_predictions_dict, verbose=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Misclassified examples - Classifier C - Label: Openness to change"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "misclassified_dict['C'][0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Misclassified examples - Classifier C - Label: Self-transcendence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "misclassified_dict['C'][1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Miscassified examples - Classifier C - Label: Self-enhancement"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "misclassified_dict['C'][2]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Misclassified examples - Classifier C - Label: Conservation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "misclassified_dict['C'][3]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Misclassified examples - Classifier CP - Label: Openness to change"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "misclassified_dict['CP'][0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Misclassified examples - Classifier CP - Label: Self-transcendence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "misclassified_dict['CP'][1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Misclassified examples - Classifier CP - Label: Self-enhancement"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "misclassified_dict['CP'][2]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Misclassified examples - Classifier CP - Label: Conservation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "misclassified_dict['CP'][3]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Misclassified examples - Classifier CPS - Label: Openness to change"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "misclassified_dict['CPS'][0] "
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Misclassified examples - Classifier CPs - Label: Self-transcendence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "misclassified_dict['CPS'][1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Misclassified examples - Classifier CPS - Label: Self-enhancement"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "misclassified_dict['CPS'][2]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Misclassified examples - Classifier CPS - Label: Conservation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "misclassified_dict['CPS'][3]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 6 - 1.0 points] Report\n",
    "\n",
    "Wrap up your experiment in a short report (up to 2 pages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Use the NLP course report template.\n",
    "* Summarize each task in the report following the provided template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Recommendations\n",
    "\n",
    "The report is not a copy-paste of graphs, tables, and command outputs.\n",
    "\n",
    "* Summarize classification performance in Table format.\n",
    "* **Do not** report command outputs or screenshots.\n",
    "* Report learning curves in Figure format.\n",
    "* The error analysis section should summarize your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Submission\n",
    "\n",
    "* **Submit** your report in PDF format.\n",
    "* **Submit** your python notebook.\n",
    "* Make sure your notebook is **well organized**, with no temporary code, commented sections, tests, etc...\n",
    "* You can upload **model weights** in a cloud repository and report the link in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# FAQ\n",
    "\n",
    "Please check this frequently asked questions before contacting us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model card\n",
    "\n",
    "You are **free** to choose the BERT-base model card you like from huggingface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model architecture\n",
    "\n",
    "You **should not** change the architecture of a model (i.e., its layers).\n",
    "\n",
    "However, you are **free** to play with their hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model Training\n",
    "\n",
    "You are **free** to choose training hyper-parameters for BERT-based models (e.g., number of epochs, etc...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Neural Libraries\n",
    "\n",
    "You are **free** to use any library of your choice to address the assignment (e.g., Keras, Tensorflow, PyTorch, JAX, etc...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Error Analysis\n",
    "\n",
    "Some topics for discussion include:\n",
    "   * Model performance on most/less frequent classes.\n",
    "   * Precision/Recall curves.\n",
    "   * Confusion matrices.\n",
    "   * Specific misclassified samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The End"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
