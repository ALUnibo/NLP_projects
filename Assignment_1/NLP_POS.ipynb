{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #1: Sequence Labeling\n",
    "## Natural Language Processing\n",
    "### Alberto Luise, Angelo Quarta, Edoardo Fusa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "import zipfile\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is obviously to download and prepare the dataset; we'll first verify if we have already downloaded the files, and if not we'll request them and unzip them. After that, we'll build a Dataframe containing all available samples, dividing them in Training, validation & Test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download folder: c:\\Users\\Alber\\Documents\\Notebooks\\NLP\n",
      "Download finished! Extracting...\n"
     ]
    }
   ],
   "source": [
    "Data_url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
    "Dataset = Data_url.split(\"/\")[-1].split(\".\")[0]\n",
    "print(f\"Download folder: {pathlib.Path.cwd()}\")\n",
    "dataset_folder = pathlib.Path.cwd().joinpath(\"Datasets\")\n",
    "\n",
    "if not dataset_folder.exists():\n",
    "    dataset_folder.mkdir(parents=True)\n",
    "if not dataset_folder.joinpath(Dataset).exists():\n",
    "    filename = dataset_folder.joinpath(\"Assignment_Dataset.zip\")\n",
    "    urllib.request.urlretrieve(url=Data_url, filename=filename)\n",
    "    print(\"Download finished! Extracting...\")\n",
    "    with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "raw_data_folder = dataset_folder.joinpath(Dataset)\n",
    "for r, set in [(range(1, 101), \"train\"), (range(101, 151), \"val\"), (range(151, 200), \"test\")]:\n",
    "    for i in r:\n",
    "        f = open(raw_data_folder.joinpath(\"wsj_\" + str(i).zfill(4) + \".dp\"),  \"r\")\n",
    "        for line in f.readlines():\n",
    "            if line != \"\\n\":\n",
    "                (word, pos, _) = line.split()\n",
    "                dataframe_row = {\n",
    "                            \"Sentence_ID\": i,\n",
    "                            \"set\": set,\n",
    "                            \"text\": word,\n",
    "                            \"POS\": pos\n",
    "                        }\n",
    "                rows.append(dataframe_row)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_folder = dataset_folder.joinpath(\"Dataframe\")\n",
    "if not df_folder.exists():\n",
    "    df_folder.mkdir(parents=True)\n",
    "df = pd.DataFrame(rows)\n",
    "df = df[[\"Sentence_ID\", \n",
    "         \"set\",\n",
    "         \"text\",\n",
    "         \"POS\"]]\n",
    "df_path = df_folder.joinpath(Dataset).with_name(Dataset + \".pkl\")\n",
    "df.to_pickle(df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Dataframe:\n",
      "       Sentence_ID    set     text  POS\n",
      "0                1  train   Pierre  NNP\n",
      "1                1  train   Vinken  NNP\n",
      "2                1  train        ,    ,\n",
      "3                1  train       61   CD\n",
      "4                1  train    years  NNS\n",
      "...            ...    ...      ...  ...\n",
      "94079          199   test  quarter   NN\n",
      "94080          199   test       of   IN\n",
      "94081          199   test     next   JJ\n",
      "94082          199   test     year   NN\n",
      "94083          199   test        .    .\n",
      "\n",
      "[94084 rows x 4 columns]\n",
      "\n",
      "Distribution of POS: \n",
      "POS\n",
      "NN       13166\n",
      "IN        9857\n",
      "NNP       9410\n",
      "DT        8165\n",
      "NNS       6047\n",
      "JJ        5834\n",
      ",         4886\n",
      ".         3874\n",
      "CD        3546\n",
      "VBD       3043\n",
      "RB        2822\n",
      "VB        2554\n",
      "CC        2265\n",
      "TO        2179\n",
      "VBN       2134\n",
      "VBZ       2125\n",
      "PRP       1716\n",
      "VBG       1460\n",
      "VBP       1321\n",
      "MD         927\n",
      "POS        824\n",
      "PRP$       766\n",
      "$          724\n",
      "``         712\n",
      "''         694\n",
      ":          563\n",
      "WDT        445\n",
      "JJR        381\n",
      "NNPS       244\n",
      "WP         241\n",
      "RP         216\n",
      "JJS        182\n",
      "WRB        178\n",
      "RBR        136\n",
      "-RRB-      126\n",
      "-LRB-      120\n",
      "EX          88\n",
      "RBS         35\n",
      "PDT         27\n",
      "#           16\n",
      "WP$         14\n",
      "LS          13\n",
      "FW           4\n",
      "UH           3\n",
      "SYM          1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribution of splits: \n",
      "set\n",
      "train    47356\n",
      "val      31183\n",
      "test     15545\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Complete Dataframe:\")\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "print(\"Distribution of POS: \")\n",
    "print(df['POS'].value_counts())\n",
    "print()\n",
    "\n",
    "print(\"Distribution of splits: \")\n",
    "print(df['set'].value_counts())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The POS column has some values (FW, UH, SYM) that are extremely rare, appearing only a single-digit number of times in 94k examples. These could reveal to be really hard to learn & recognize, we'll see if some data augmentation is in order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our machine only understands numbers, we have to embed the words before using them as input data. We'll use GloVe Embedding (Global Vectors), an unsupervised algorithm based on co-occurrence statistics from a specific corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
