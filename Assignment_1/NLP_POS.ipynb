{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #1: Sequence Labeling\n",
    "## Natural Language Processing\n",
    "### Alberto Luise, Angelo Quarta, Edoardo Fusa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "import zipfile\n",
    "import os\n",
    "import collections\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is obviously to download and prepare the dataset; we'll first verify if we have already downloaded the files, and if not we'll request them and unzip them. After that, we'll build a Dataframe containing all available samples, dividing them in Training, validation & Test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download folder: c:\\Users\\Alber\\Documents\\Notebooks\\Assignment_1\n"
     ]
    }
   ],
   "source": [
    "Data_url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
    "Dataset = Data_url.split(\"/\")[-1].split(\".\")[0]\n",
    "print(f\"Download folder: {pathlib.Path.cwd()}\")\n",
    "dataset_folder = pathlib.Path.cwd().joinpath(\"Datasets\")\n",
    "\n",
    "if not dataset_folder.exists():\n",
    "    dataset_folder.mkdir(parents=True)\n",
    "if not dataset_folder.joinpath(Dataset).exists():\n",
    "    filename = dataset_folder.joinpath(\"Assignment_Dataset.zip\")\n",
    "    urllib.request.urlretrieve(url=Data_url, filename=filename)\n",
    "    print(\"Download finished! Extracting...\")\n",
    "    with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "raw_data_folder = dataset_folder.joinpath(Dataset)\n",
    "for r, set in [(range(1, 101), \"train\"), (range(101, 151), \"val\"), (range(151, 200), \"test\")]:\n",
    "    for i in r:\n",
    "        f = open(raw_data_folder.joinpath(\"wsj_\" + str(i).zfill(4) + \".dp\"),  \"r\")\n",
    "        for line in f.readlines():\n",
    "            if line != \"\\n\":\n",
    "                (word, pos, _) = line.split()\n",
    "                dataframe_row = {\n",
    "                            \"Sentence_ID\": i,\n",
    "                            \"set\": set,\n",
    "                            \"text\": word,\n",
    "                            \"POS\": pos\n",
    "                        }\n",
    "                rows.append(dataframe_row)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_folder = dataset_folder.joinpath(\"Dataframe\")\n",
    "if not df_folder.exists():\n",
    "    df_folder.mkdir(parents=True)\n",
    "df = pd.DataFrame(rows)\n",
    "df = df[[\"Sentence_ID\", \n",
    "         \"set\",\n",
    "         \"text\",\n",
    "         \"POS\"]]\n",
    "df_path = df_folder.joinpath(Dataset).with_name(Dataset + \".pkl\")\n",
    "df.to_pickle(df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Dataframe:\n",
      "       Sentence_ID    set     text  POS\n",
      "0                1  train   Pierre  NNP\n",
      "1                1  train   Vinken  NNP\n",
      "2                1  train        ,    ,\n",
      "3                1  train       61   CD\n",
      "4                1  train    years  NNS\n",
      "...            ...    ...      ...  ...\n",
      "94079          199   test  quarter   NN\n",
      "94080          199   test       of   IN\n",
      "94081          199   test     next   JJ\n",
      "94082          199   test     year   NN\n",
      "94083          199   test        .    .\n",
      "\n",
      "[94084 rows x 4 columns]\n",
      "\n",
      "Distribution of POS: \n",
      "POS\n",
      "NN       13166\n",
      "IN        9857\n",
      "NNP       9410\n",
      "DT        8165\n",
      "NNS       6047\n",
      "JJ        5834\n",
      ",         4886\n",
      ".         3874\n",
      "CD        3546\n",
      "VBD       3043\n",
      "RB        2822\n",
      "VB        2554\n",
      "CC        2265\n",
      "TO        2179\n",
      "VBN       2134\n",
      "VBZ       2125\n",
      "PRP       1716\n",
      "VBG       1460\n",
      "VBP       1321\n",
      "MD         927\n",
      "POS        824\n",
      "PRP$       766\n",
      "$          724\n",
      "``         712\n",
      "''         694\n",
      ":          563\n",
      "WDT        445\n",
      "JJR        381\n",
      "NNPS       244\n",
      "WP         241\n",
      "RP         216\n",
      "JJS        182\n",
      "WRB        178\n",
      "RBR        136\n",
      "-RRB-      126\n",
      "-LRB-      120\n",
      "EX          88\n",
      "RBS         35\n",
      "PDT         27\n",
      "#           16\n",
      "WP$         14\n",
      "LS          13\n",
      "FW           4\n",
      "UH           3\n",
      "SYM          1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribution of splits: \n",
      "set\n",
      "train    47356\n",
      "val      31183\n",
      "test     15545\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Complete Dataframe:\")\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "print(\"Distribution of POS: \")\n",
    "print(df['POS'].value_counts())\n",
    "print()\n",
    "\n",
    "print(\"Distribution of splits: \")\n",
    "print(df['set'].value_counts())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The POS column has some values (FW, UH, SYM) that are extremely rare, appearing only a single-digit number of times in 94k examples. These could reveal to be really hard to learn & recognize, we'll see if some data augmentation is in order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our machine only understands numbers, we have to embed the words before using them as input data. We'll use GloVe Embedding (Global Vectors), an unsupervised algorithm based on co-occurrence statistics from a specific corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Creating a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_symbols(s):\n",
    "   return not all(i in string.punctuation for i in s)\n",
    "\n",
    "df['text'] = df['text'].str.lower()\n",
    "df['text'] = df['text'].str.strip()\n",
    "mask = df['text'].apply(exclude_symbols)\n",
    "df = df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Debug] Index -> Word vocabulary size: 10929\n",
      "[Debug] Word -> Index vocabulary size: 10929\n",
      "[Debug] Some words: [('pierre', 0), ('vinken', 1), ('61', 2), ('years', 3), ('old', 4), ('will', 5), ('join', 6), ('the', 7), ('board', 8), ('as', 9)]\n"
     ]
    }
   ],
   "source": [
    "def build_vocabulary(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Function defining a vocabulary, returning both an 'Index-to-Word' version and a 'Word-to-Index' version.\n",
    "    Done temporarily to test everything, could be improved or cleaned up.\n",
    "    \"\"\"\n",
    "    Index_to_Word = collections.OrderedDict()\n",
    "    Word_to_Index = collections.OrderedDict()\n",
    "    \n",
    "    idx = 0\n",
    "    for sentence in df.text.values:\n",
    "        tokens = sentence.split()\n",
    "        for token in tokens:\n",
    "            if token not in Word_to_Index:\n",
    "                Word_to_Index[token] = idx\n",
    "                Index_to_Word[idx] = token\n",
    "                idx += 1\n",
    "\n",
    "    return Index_to_Word, Word_to_Index\n",
    "\n",
    "\n",
    "idx_to_word, word_to_idx = build_vocabulary(df)\n",
    "print(f'[Debug] Index -> Word vocabulary size: {len(idx_to_word)}')\n",
    "print(f'[Debug] Word -> Index vocabulary size: {len(word_to_idx)}')\n",
    "print(f'[Debug] Some words: {[(idx_to_word[idx], idx) for idx in np.arange(10)]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving vocabulary to c:\\Users\\Alber\\Documents\\Notebooks\\Assignment_1\\Datasets\\Vocabulary.json\n",
      "Saving completed!\n"
     ]
    }
   ],
   "source": [
    "# Optional: save the vocabulary to a JSON for manual checking\n",
    "SAVE_VOC = True\n",
    "\n",
    "if SAVE_VOC:\n",
    "    import simplejson as sj\n",
    "\n",
    "    vocab_path = pathlib.Path.cwd().joinpath('Datasets', 'Vocabulary.json')\n",
    "\n",
    "    print(f\"Saving vocabulary to {vocab_path}\")\n",
    "    with vocab_path.open(mode='w') as f:\n",
    "        sj.dump(word_to_idx, f, indent=4)\n",
    "    print(\"Saving completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Applying Embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
